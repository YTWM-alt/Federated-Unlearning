
Experimental details:
    Model     : allcnn
    Optimizer : adam
    Learning  : 0.001
    Global Rounds   : 40

    Federated parameters:
    Non-IID
    Number of participating users  : -1
    Batch size   : 64
    Local Epochs       : 1

Files already downloaded and verified
Files already downloaded and verified
Data distribution for client : 0 :::: {0: 40, 1: 61, 2: 81, 3: 327, 4: 308, 5: 343, 6: 34, 7: 972, 8: 1, 9: 3412}
Data distribution for client : 1 :::: {0: 329, 1: 182, 2: 357, 3: 5, 4: 3063, 5: 0, 6: 1108, 7: 0, 8: 0, 9: 0}
Data distribution for client : 2 :::: {0: 45, 1: 1113, 2: 203, 3: 4, 4: 911, 5: 9, 6: 520, 7: 8, 8: 40, 9: 70}
Data distribution for client : 3 :::: {0: 1266, 1: 545, 2: 12, 3: 139, 4: 0, 5: 22, 6: 380, 7: 25, 8: 37, 9: 1016}
Data distribution for client : 4 :::: {0: 2148, 1: 11, 2: 1687, 3: 26, 4: 14, 5: 854, 6: 116, 7: 4, 8: 1460, 9: 0}
Data distribution for client : 5 :::: {0: 101, 1: 350, 2: 907, 3: 2404, 4: 58, 5: 188, 6: 28, 7: 19, 8: 3, 9: 61}
Data distribution for client : 6 :::: {0: 81, 1: 3, 2: 81, 3: 174, 4: 104, 5: 1951, 6: 1507, 7: 3516, 8: 0, 9: 0}
Data distribution for client : 7 :::: {0: 85, 1: 2304, 2: 322, 3: 13, 4: 134, 5: 143, 6: 149, 7: 311, 8: 645, 9: 21}
Data distribution for client : 8 :::: {0: 437, 1: 292, 2: 41, 3: 93, 4: 398, 5: 718, 6: 1117, 7: 109, 8: 319, 9: 419}
Data distribution for client : 9 :::: {0: 468, 1: 139, 2: 1309, 3: 1815, 4: 10, 5: 772, 6: 41, 7: 36, 8: 2495, 9: 1}
Creating data loader for client: 0
Creating data loader for client: 1
Creating data loader for client: 2
Creating data loader for client: 3
Creating data loader for client: 4
Creating data loader for client: 5
Creating data loader for client: 6
Creating data loader for client: 7
Creating data loader for client: 8
Creating data loader for client: 9
Model: AllCNN(
  (conv1): Conv(
    (0): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (conv2): Conv(
    (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (conv3): Conv(
    (0): Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (dropout1): Sequential(
    (0): Identity()
  )
  (features): Sequential(
    (0): Identity()
  )
  (conv4): Conv(
    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (conv5): Conv(
    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (conv6): Conv(
    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (dropout2): Sequential(
    (0): Identity()
  )
  (conv7): Conv(
    (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (conv8): Conv(
    (0): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU()
  )
  (pool): AdaptiveAvgPool2d(output_size=(2, 2))
  (flatten): Flatten()
  (classifier): Sequential(
    (0): Linear(in_features=768, out_features=10, bias=True)
  )
)
Global Iteration: 0
Client: 0
Epoch 0 [1/88] loss=2.4288
Epoch 0 [2/88] loss=1.5283
Epoch 0 [3/88] loss=1.1090
Epoch 0 [4/88] loss=1.1304
Epoch 0 [5/88] loss=1.2983
Epoch 0 [6/88] loss=0.9774
Epoch 0 [7/88] loss=1.0195
Epoch 0 [8/88] loss=0.6898
Epoch 0 [9/88] loss=0.7636
Epoch 0 [10/88] loss=0.9879
Epoch 0 [11/88] loss=0.8377
Epoch 0 [12/88] loss=0.9628
Epoch 0 [13/88] loss=0.8804
Epoch 0 [14/88] loss=0.9707
Epoch 0 [15/88] loss=0.6818
Epoch 0 [16/88] loss=0.8523
Epoch 0 [17/88] loss=1.0625
Epoch 0 [18/88] loss=0.9837
Epoch 0 [19/88] loss=1.0843
Epoch 0 [20/88] loss=0.8083
Epoch 0 [21/88] loss=0.7256
Epoch 0 [22/88] loss=1.0052
Epoch 0 [23/88] loss=0.8174
Epoch 0 [24/88] loss=1.0166
Epoch 0 [25/88] loss=0.7623
Epoch 0 [26/88] loss=1.0631
Epoch 0 [27/88] loss=0.8799
Epoch 0 [28/88] loss=0.8441
Epoch 0 [29/88] loss=0.7224
Epoch 0 [30/88] loss=1.3863
Epoch 0 [31/88] loss=0.9222
Epoch 0 [32/88] loss=1.0818
Epoch 0 [33/88] loss=0.9413
Epoch 0 [34/88] loss=0.9635
Epoch 0 [35/88] loss=0.7349
Epoch 0 [36/88] loss=0.9093
Epoch 0 [37/88] loss=0.9473
Epoch 0 [38/88] loss=0.8937
Epoch 0 [39/88] loss=1.2299
Epoch 0 [40/88] loss=1.0250
Epoch 0 [41/88] loss=0.7138
Epoch 0 [42/88] loss=0.7799
Epoch 0 [43/88] loss=1.0059
Epoch 0 [44/88] loss=1.1137
Epoch 0 [45/88] loss=0.8193
Epoch 0 [46/88] loss=0.9055
Epoch 0 [47/88] loss=0.8155
Epoch 0 [48/88] loss=0.9698
Epoch 0 [49/88] loss=1.0029
Epoch 0 [50/88] loss=0.7265
Epoch 0 [51/88] loss=0.8543
Epoch 0 [52/88] loss=0.8030
Epoch 0 [53/88] loss=0.5732
Epoch 0 [54/88] loss=0.8961
Epoch 0 [55/88] loss=0.6705
Epoch 0 [56/88] loss=0.8289
Epoch 0 [57/88] loss=1.1130
Epoch 0 [58/88] loss=0.8023
Epoch 0 [59/88] loss=0.7313
Epoch 0 [60/88] loss=0.9569
Epoch 0 [61/88] loss=0.9012
Epoch 0 [62/88] loss=0.8901
Epoch 0 [63/88] loss=0.7672
Epoch 0 [64/88] loss=1.1018
Epoch 0 [65/88] loss=0.9573
Epoch 0 [66/88] loss=0.7371
Epoch 0 [67/88] loss=0.6817
Epoch 0 [68/88] loss=0.6543
Epoch 0 [69/88] loss=0.8666
Epoch 0 [70/88] loss=0.6685
Epoch 0 [71/88] loss=0.7330
Epoch 0 [72/88] loss=1.0075
Epoch 0 [73/88] loss=0.8256
Epoch 0 [74/88] loss=0.8477
Epoch 0 [75/88] loss=0.8930
Epoch 0 [76/88] loss=0.9540
Epoch 0 [77/88] loss=0.6436
Epoch 0 [78/88] loss=0.7187
Epoch 0 [79/88] loss=0.8516
Epoch 0 [80/88] loss=0.5371
Epoch 0 [81/88] loss=0.8103
Epoch 0 [82/88] loss=0.6879
Epoch 0 [83/88] loss=0.7355
Epoch 0 [84/88] loss=0.9509
Epoch 0 [85/88] loss=0.8545
Epoch 0 [86/88] loss=0.7661
Epoch 0 [87/88] loss=0.8439
Epoch 0 [88/88] loss=0.8897
Client: 1
Epoch 0 [1/79] loss=2.1043
Epoch 0 [2/79] loss=1.4018
Epoch 0 [3/79] loss=1.2897
Epoch 0 [4/79] loss=1.1273
Epoch 0 [5/79] loss=1.0583
Epoch 0 [6/79] loss=1.0373
Epoch 0 [7/79] loss=1.1920
Epoch 0 [8/79] loss=1.1279
Epoch 0 [9/79] loss=0.9728
Epoch 0 [10/79] loss=1.0103
Epoch 0 [11/79] loss=1.1019
Epoch 0 [12/79] loss=1.0983
Epoch 0 [13/79] loss=0.9962
Epoch 0 [14/79] loss=1.1167
Epoch 0 [15/79] loss=0.9369
Epoch 0 [16/79] loss=1.1514
Epoch 0 [17/79] loss=0.9420
Epoch 0 [18/79] loss=0.9103
Epoch 0 [19/79] loss=0.9944
Epoch 0 [20/79] loss=1.1097
Epoch 0 [21/79] loss=0.8243
Epoch 0 [22/79] loss=0.8076
Epoch 0 [23/79] loss=0.8329
Epoch 0 [24/79] loss=0.8214
Epoch 0 [25/79] loss=0.9741
Epoch 0 [26/79] loss=0.8192
Epoch 0 [27/79] loss=0.7962
Epoch 0 [28/79] loss=1.0061
Epoch 0 [29/79] loss=0.6929
Epoch 0 [30/79] loss=1.0340
Epoch 0 [31/79] loss=0.9318
Epoch 0 [32/79] loss=0.8933
Epoch 0 [33/79] loss=1.1864
Epoch 0 [34/79] loss=0.7758
Epoch 0 [35/79] loss=1.0940
Epoch 0 [36/79] loss=0.8737
Epoch 0 [37/79] loss=0.8697
Epoch 0 [38/79] loss=0.8132
Epoch 0 [39/79] loss=0.7789
Epoch 0 [40/79] loss=0.8390
Epoch 0 [41/79] loss=0.9613
Epoch 0 [42/79] loss=0.9692
Epoch 0 [43/79] loss=0.7617
Epoch 0 [44/79] loss=0.9884
Epoch 0 [45/79] loss=1.0462
Epoch 0 [46/79] loss=0.8286
Epoch 0 [47/79] loss=1.0239
Epoch 0 [48/79] loss=0.8745
Epoch 0 [49/79] loss=1.0169
Epoch 0 [50/79] loss=0.9739
Epoch 0 [51/79] loss=0.8618
Epoch 0 [52/79] loss=0.8648
Epoch 0 [53/79] loss=0.7017
Epoch 0 [54/79] loss=0.7708
Epoch 0 [55/79] loss=0.8378
Epoch 0 [56/79] loss=0.8874
Epoch 0 [57/79] loss=0.9255
Epoch 0 [58/79] loss=0.7887
Epoch 0 [59/79] loss=1.0705
Epoch 0 [60/79] loss=1.0803
Epoch 0 [61/79] loss=0.7206
Epoch 0 [62/79] loss=0.7986
Epoch 0 [63/79] loss=0.6877
Epoch 0 [64/79] loss=0.5942
Epoch 0 [65/79] loss=0.9413
Epoch 0 [66/79] loss=0.9349
Epoch 0 [67/79] loss=0.9762
Epoch 0 [68/79] loss=0.8233
Epoch 0 [69/79] loss=1.0165
Epoch 0 [70/79] loss=0.7236
Epoch 0 [71/79] loss=0.6787
Epoch 0 [72/79] loss=0.7561
Epoch 0 [73/79] loss=0.8478
Epoch 0 [74/79] loss=0.7468
Epoch 0 [75/79] loss=0.7917
Epoch 0 [76/79] loss=0.7796
Epoch 0 [77/79] loss=0.9199
Epoch 0 [78/79] loss=0.7554
Epoch 0 [79/79] loss=0.6833
Client: 2
Epoch 0 [1/46] loss=2.2776
Epoch 0 [2/46] loss=1.6703
Epoch 0 [3/46] loss=1.5267
Epoch 0 [4/46] loss=1.1800
Epoch 0 [5/46] loss=1.0227
Epoch 0 [6/46] loss=1.2677
Epoch 0 [7/46] loss=1.1480
Epoch 0 [8/46] loss=0.9423
Epoch 0 [9/46] loss=0.8879
Epoch 0 [10/46] loss=0.9906
Epoch 0 [11/46] loss=1.0723
Epoch 0 [12/46] loss=0.9693
Epoch 0 [13/46] loss=1.1645
Epoch 0 [14/46] loss=1.0900
Epoch 0 [15/46] loss=0.9542
Epoch 0 [16/46] loss=1.1315
Epoch 0 [17/46] loss=1.0667
Epoch 0 [18/46] loss=1.0089
Epoch 0 [19/46] loss=0.8454
Epoch 0 [20/46] loss=0.9703
Epoch 0 [21/46] loss=1.2546
Epoch 0 [22/46] loss=0.9714
Epoch 0 [23/46] loss=1.0150
Epoch 0 [24/46] loss=1.1763
Epoch 0 [25/46] loss=0.9785
Epoch 0 [26/46] loss=1.0929
Epoch 0 [27/46] loss=1.0522
